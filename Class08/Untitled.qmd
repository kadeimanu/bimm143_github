---
title: "Class08"
author: "Kaliyah Adei-Manu"
format: pdf
---

# Background

In today's class we will apply the emthods and techniques clustering and PCA to help make sense of real world breast cancer FNA biopsy data set 

## Data import 

We will start by importikng our data.  It is a CSV file so we will use the `read.csv()` function.

```{r}
fna.data <- "https://bioboot.github.io/bimm143_S20/class-material/WisconsinCancer.csv"

wisc.df <- read.csv(fna.data, row.name=1)
```

We will have a look at the first few rows 

```{r}
head(wisc.df)
```
Make sure to remove the first `diagnosis` column - I don't want to use this for my machine learning models.  We will use it later to compare our results to the expert diagnosis.  

```{r}
wisc.data <- wisc.df[,-1]
diagnosis <- wisc.df$diagnosis
```

> Q1. How many observations are in this dataset?

```{r}
nrow(wisc.data)
```
> Q2. How many of the observations have a malignant diagnosis?

```{r}
table(diagnosis)
```
212 have a malignant diagnosis 

> Q3. How many variables/features in the data are suffixed with _mean?

```{r}
colnames <- colnames(wisc.data)
length(grep("_mean", colnames))
```
There are 10 columns that are suffixed `_mean`

## Principal Component Analysis
The main function here is `prcomp()` and we want to make sure we set the optional argument `scale=TRUE` 
```{r}
wisc.pr <- prcomp(wisc.data, scale=TRUE)
summary(wisc.pr)
```
> Q4. From your results, what proportion of the original variance is captured by the first principal component (PC1)?

The first principal component captures 44.27% of the variance. 

> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

3 PC's are required to describe 70% of the original variance, the 3rd PC   cumulatively captures 72.6% of the original variance.

> Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

7 Pc's are needed to capture at least 90% of the original variance.  PCA 7 collectively accounts for 91.0% of the original variance. 

```{r}
 biplot(wisc.pr)
```
> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

Its quite difficult to read a lot of the information and data overlaps making it difficult to interpret. 

Our main PCA "score plot" or "PC plot" of results: 

```{r}
library(ggplot2)
```
```{r}
ggplot(wisc.pr$x)+ 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()
```

> Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
ggplot(wisc.pr$x) +
  aes(PC1, PC3, col=diagnosis) +
  geom_point()
```
This plot has more overlap of the two clusters than the plot of PC1 vs PC2, this plot doesn't appear to have a linear relationship. 
## Interpreting PCA results


Variance Explained 

```{r}
pr.var <- wisc.pr$sdev^2
head(pr.var)
```
```{r}
pve <- pr.var/30
plot(c(1,pve), xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```
An Alternative Scree Plot 
```{r}
barplot(pve, ylab = "Percent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```
Communicating PCA Results:

> Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean? This tells us how much this original feature contributes to the first PC. Are there any features with larger contributions than this one?

```{r}
wisc.pr$rotation[,1]
```
 The `concave.point_mean value is  -0.26085376, this shows how much this contributes to PCA 1


## Hierarchial Clustering

First scale the data (with the `scale()` function), then calculate a distance matrix (with the `dist()` function. Then cluster with the `hclust()` function and plot: 

```{r}
wisc.hclust <- hclust(dist(scale(wisc.data)))
```


> Q10. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

```{r}
plot(wisc.hclust)
abline(h=19, col="red", lyt= 2)
```

You can also use the `cutree()`

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)
table(wisc.hclust.clusters)
```

## Combining methods 

here we will take our PCA results and use those as input for clustering.  In other words our `wisc.pr$x` scores that we plotted above (the main output from PCA - how the data lies on our new principal component axis/variables) and use a subset of these PCs that capture the most variance as input for `hclust()`


```{r}
pc.dist <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust(pc.dist, method="average")
plot(wisc.pr.hclust)
```


```{r}
pc.dist <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust(pc.dist, method="ward.D2")
plot(wisc.pr.hclust)
```
> Q12. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

I liked "ward.D2" , for this dendrogram I think it made more clear clusters then "average"

Let's find out if these two clusters are malignant and benign

Cut the dendrogram/ tree into two main groups/ clusters: 

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```
I want to know how many of the clustering `grps` with values of1 or 2 correspond to the expert `diagnosis`

> Q. Q13. How well does the newly created hclust model with two clusters separate out the two “M” and “B” diagnoses?

```{r}
table(grps,diagnosis)
```
>Q14. How well do the hierarchical clustering models you created in the previous sections (i.e. without first doing PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.hclust.clusters and wisc.pr.hclust.clusters) with the vector containing the actual diagnoses.

```{r}
table(wisc.hclust.clusters, diagnosis)
```


```{r}
ggplot(wisc.pr$x) +
  aes(PC1, PC2) +
  geom_point(col=grps)
```

My clustering **group 1** are mostly "M" diagnosis (179) and my clustering **group 2** are mostly "B: diagnosis 

24 FP
179 TP 
333 TN
33 FN

Sensitivity TP/(TP+FN)

```{r}
179/(179+33)
```

Specificity TN/(TN/FP)
```{r}
333/(333+24)
```

 Use this PCA model to predict using data from Michigan: 
 
```{r}
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

Making a new PCA plot for Michigan : 
```{r}
plot(wisc.pr$x[,1:2], col=grps)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```
 >  Q16. Which of these new patients should we prioritize for follow up based on your results?
 
 We should prioritize analyzing the patients that overlap.  In other words prioritize follow up for patients where there are both black and red dots, meaning these patients are more likely to be misdiagnosed than the patients wwho are closer to the centers of their clusters. 
